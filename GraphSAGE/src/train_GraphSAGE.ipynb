{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "from sampling import multihop_sampling\n",
    "from model import GraphSAGE\n",
    "from dataset import CoraData\n",
    "from collections import namedtuple\n",
    "from config import DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_torch(seed=0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:2\n"
     ]
    }
   ],
   "source": [
    "INPUT_DIM = 1433    # 每篇论文的词向量长度\n",
    "HIDDEN_DIM = [128, 7]   # 隐藏层维度\n",
    "NUM_NEIGHBORS_LIST = [20, 20]   # 邻居节点列表\n",
    "assert len(HIDDEN_DIM) == len(NUM_NEIGHBORS_LIST)   # 判断隐藏层层数是否和采样邻居次数一致\n",
    "BTACH_SIZE = 4 # batch size\n",
    "EPOCHS = 30 # 迭代次数\n",
    "NUM_BATCH_PER_EPOCH = 10    # 每个epoch中的batch个数\n",
    "LEARNING_RATE = 0.001   # 学习率\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Cached file: ../../GCN/Cora/processed_cora.pkl\n"
     ]
    }
   ],
   "source": [
    "Data = namedtuple(\n",
    "    'Data',\n",
    "    ['x', 'y', 'adjacency_dict', 'train_mask', 'val_mask', 'test_mask'])\n",
    "\n",
    "data = CoraData(data_root='../../GCN/Cora').data\n",
    "x = data.x / data.x.sum(1, keepdims=True)  # 归一化数据，使得每一行和为1\n",
    "\n",
    "train_index = np.where(data.train_mask)[0]\n",
    "train_label = data.y\n",
    "test_index = np.where(data.test_mask)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggre_method=\"LSTM\" # mean,sum,max\n",
    "model = GraphSAGE(input_dim=INPUT_DIM,\n",
    "                  hidden_dim=HIDDEN_DIM,\n",
    "                  num_neighbors_list=NUM_NEIGHBORS_LIST,aggre_method=aggre_method).to(DEVICE)\n",
    "# print(model)\n",
    "criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    acc_list = []\n",
    "    for e in range(EPOCHS):\n",
    "        for batch in range(NUM_BATCH_PER_EPOCH):\n",
    "            batch_src_index = np.random.choice(train_index,\n",
    "                                               size=(BTACH_SIZE, ))\n",
    "            batch_src_label = torch.from_numpy(\n",
    "                train_label[batch_src_index]).long().to(DEVICE)\n",
    "            batch_sampling_result = multihop_sampling(batch_src_index,\n",
    "                                                      NUM_NEIGHBORS_LIST,\n",
    "                                                      data.adjacency_dict)\n",
    "            batch_sampling_x = [\n",
    "                torch.from_numpy(x[idx]).float().to(DEVICE)\n",
    "                for idx in batch_sampling_result\n",
    "            ]\n",
    "            batch_train_logits = model(batch_sampling_x)\n",
    "            loss = criterion(batch_train_logits, batch_src_label)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print(\"Epoch {:03d} Batch {:03d} Loss: {:.4f}\".format(\n",
    "                e, batch, loss.item()))\n",
    "        # acc = test()\n",
    "        # acc_list.append(acc)\n",
    "    return acc_list\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_sampling_result = multihop_sampling(test_index,\n",
    "                                                 NUM_NEIGHBORS_LIST,\n",
    "                                                 data.adjacency_dict)\n",
    "        test_x = [\n",
    "            torch.from_numpy(x[idx]).float().to(DEVICE)\n",
    "            for idx in test_sampling_result\n",
    "        ]\n",
    "        test_logits = model(test_x)\n",
    "        test_label = torch.from_numpy(data.y[test_index]).long().to(DEVICE)\n",
    "        predict_y = test_logits.max(1)[1]\n",
    "        accuarcy = torch.eq(predict_y, test_label).float().mean().item()\n",
    "        print(\"Test Accuracy: \", accuarcy)\n",
    "        return accuarcy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000 Batch 000 Loss: 1.8196\n",
      "Epoch 000 Batch 001 Loss: 1.8666\n",
      "Epoch 000 Batch 002 Loss: 1.7380\n",
      "Epoch 000 Batch 003 Loss: 2.1745\n",
      "Epoch 000 Batch 004 Loss: 1.6423\n",
      "Epoch 000 Batch 005 Loss: 1.9176\n",
      "Epoch 000 Batch 006 Loss: 2.1748\n",
      "Epoch 000 Batch 007 Loss: 1.8732\n",
      "Epoch 000 Batch 008 Loss: 2.3153\n",
      "Epoch 000 Batch 009 Loss: 2.2633\n",
      "Epoch 001 Batch 000 Loss: 2.0498\n",
      "Epoch 001 Batch 001 Loss: 2.0913\n",
      "Epoch 001 Batch 002 Loss: 2.0688\n",
      "Epoch 001 Batch 003 Loss: 1.6306\n",
      "Epoch 001 Batch 004 Loss: 1.9283\n",
      "Epoch 001 Batch 005 Loss: 2.2237\n",
      "Epoch 001 Batch 006 Loss: 1.8273\n",
      "Epoch 001 Batch 007 Loss: 1.9530\n",
      "Epoch 001 Batch 008 Loss: 1.9450\n",
      "Epoch 001 Batch 009 Loss: 1.8295\n",
      "Epoch 002 Batch 000 Loss: 2.1383\n",
      "Epoch 002 Batch 001 Loss: 2.1227\n",
      "Epoch 002 Batch 002 Loss: 2.2472\n",
      "Epoch 002 Batch 003 Loss: 1.6584\n",
      "Epoch 002 Batch 004 Loss: 2.2153\n",
      "Epoch 002 Batch 005 Loss: 1.9793\n",
      "Epoch 002 Batch 006 Loss: 2.3397\n",
      "Epoch 002 Batch 007 Loss: 2.1333\n",
      "Epoch 002 Batch 008 Loss: 2.0890\n",
      "Epoch 002 Batch 009 Loss: 1.9502\n",
      "Epoch 003 Batch 000 Loss: 1.7559\n",
      "Epoch 003 Batch 001 Loss: 2.1273\n",
      "Epoch 003 Batch 002 Loss: 1.8321\n",
      "Epoch 003 Batch 003 Loss: 1.9834\n",
      "Epoch 003 Batch 004 Loss: 1.9377\n",
      "Epoch 003 Batch 005 Loss: 1.3363\n",
      "Epoch 003 Batch 006 Loss: 2.1905\n",
      "Epoch 003 Batch 007 Loss: 1.9481\n",
      "Epoch 003 Batch 008 Loss: 1.6550\n",
      "Epoch 003 Batch 009 Loss: 1.7619\n",
      "Epoch 004 Batch 000 Loss: 1.9203\n",
      "Epoch 004 Batch 001 Loss: 1.5974\n",
      "Epoch 004 Batch 002 Loss: 1.7461\n",
      "Epoch 004 Batch 003 Loss: 1.9405\n",
      "Epoch 004 Batch 004 Loss: 1.8084\n",
      "Epoch 004 Batch 005 Loss: 2.0337\n",
      "Epoch 004 Batch 006 Loss: 1.8089\n",
      "Epoch 004 Batch 007 Loss: 2.5882\n",
      "Epoch 004 Batch 008 Loss: 1.9963\n",
      "Epoch 004 Batch 009 Loss: 1.8962\n",
      "Epoch 005 Batch 000 Loss: 1.7789\n",
      "Epoch 005 Batch 001 Loss: 2.0006\n",
      "Epoch 005 Batch 002 Loss: 1.9060\n",
      "Epoch 005 Batch 003 Loss: 2.0247\n",
      "Epoch 005 Batch 004 Loss: 1.7628\n",
      "Epoch 005 Batch 005 Loss: 1.8263\n",
      "Epoch 005 Batch 006 Loss: 1.9942\n",
      "Epoch 005 Batch 007 Loss: 1.8796\n",
      "Epoch 005 Batch 008 Loss: 1.8412\n",
      "Epoch 005 Batch 009 Loss: 2.0666\n",
      "Epoch 006 Batch 000 Loss: 2.1886\n",
      "Epoch 006 Batch 001 Loss: 1.7247\n",
      "Epoch 006 Batch 002 Loss: 1.7963\n",
      "Epoch 006 Batch 003 Loss: 2.1519\n",
      "Epoch 006 Batch 004 Loss: 1.7520\n",
      "Epoch 006 Batch 005 Loss: 1.9969\n",
      "Epoch 006 Batch 006 Loss: 1.7503\n",
      "Epoch 006 Batch 007 Loss: 1.5904\n",
      "Epoch 006 Batch 008 Loss: 1.6717\n",
      "Epoch 006 Batch 009 Loss: 1.8224\n",
      "Epoch 007 Batch 000 Loss: 2.8972\n",
      "Epoch 007 Batch 001 Loss: 1.9332\n",
      "Epoch 007 Batch 002 Loss: 2.0552\n",
      "Epoch 007 Batch 003 Loss: 1.9538\n",
      "Epoch 007 Batch 004 Loss: 1.8409\n",
      "Epoch 007 Batch 005 Loss: 1.8140\n",
      "Epoch 007 Batch 006 Loss: 1.9402\n",
      "Epoch 007 Batch 007 Loss: 1.9950\n",
      "Epoch 007 Batch 008 Loss: 1.9611\n",
      "Epoch 007 Batch 009 Loss: 2.1925\n",
      "Epoch 008 Batch 000 Loss: 1.5989\n",
      "Epoch 008 Batch 001 Loss: 1.4082\n",
      "Epoch 008 Batch 002 Loss: 2.0594\n",
      "Epoch 008 Batch 003 Loss: 1.5903\n",
      "Epoch 008 Batch 004 Loss: 2.1865\n",
      "Epoch 008 Batch 005 Loss: 1.9271\n",
      "Epoch 008 Batch 006 Loss: 1.8435\n",
      "Epoch 008 Batch 007 Loss: 1.8409\n",
      "Epoch 008 Batch 008 Loss: 1.8620\n",
      "Epoch 008 Batch 009 Loss: 1.6649\n",
      "Epoch 009 Batch 000 Loss: 1.5860\n",
      "Epoch 009 Batch 001 Loss: 1.7952\n",
      "Epoch 009 Batch 002 Loss: 1.6817\n",
      "Epoch 009 Batch 003 Loss: 2.0303\n",
      "Epoch 009 Batch 004 Loss: 1.9007\n",
      "Epoch 009 Batch 005 Loss: 1.4385\n",
      "Epoch 009 Batch 006 Loss: 1.8185\n",
      "Epoch 009 Batch 007 Loss: 1.6487\n",
      "Epoch 009 Batch 008 Loss: 2.0262\n",
      "Epoch 009 Batch 009 Loss: 1.8125\n",
      "Epoch 010 Batch 000 Loss: 1.8033\n",
      "Epoch 010 Batch 001 Loss: 1.7922\n",
      "Epoch 010 Batch 002 Loss: 1.9646\n",
      "Epoch 010 Batch 003 Loss: 1.6288\n",
      "Epoch 010 Batch 004 Loss: 1.6183\n",
      "Epoch 010 Batch 005 Loss: 1.8380\n",
      "Epoch 010 Batch 006 Loss: 1.7233\n",
      "Epoch 010 Batch 007 Loss: 1.8858\n",
      "Epoch 010 Batch 008 Loss: 1.5468\n",
      "Epoch 010 Batch 009 Loss: 1.7172\n",
      "Epoch 011 Batch 000 Loss: 2.0432\n",
      "Epoch 011 Batch 001 Loss: 2.0140\n",
      "Epoch 011 Batch 002 Loss: 2.0320\n",
      "Epoch 011 Batch 003 Loss: 2.1551\n",
      "Epoch 011 Batch 004 Loss: 1.8687\n",
      "Epoch 011 Batch 005 Loss: 1.8232\n",
      "Epoch 011 Batch 006 Loss: 1.4404\n",
      "Epoch 011 Batch 007 Loss: 1.7418\n",
      "Epoch 011 Batch 008 Loss: 1.8860\n",
      "Epoch 011 Batch 009 Loss: 1.8569\n",
      "Epoch 012 Batch 000 Loss: 1.7486\n",
      "Epoch 012 Batch 001 Loss: 1.7501\n",
      "Epoch 012 Batch 002 Loss: 1.5412\n",
      "Epoch 012 Batch 003 Loss: 1.5374\n",
      "Epoch 012 Batch 004 Loss: 2.0783\n",
      "Epoch 012 Batch 005 Loss: 1.8347\n",
      "Epoch 012 Batch 006 Loss: 1.8765\n",
      "Epoch 012 Batch 007 Loss: 1.7913\n",
      "Epoch 012 Batch 008 Loss: 1.7347\n",
      "Epoch 012 Batch 009 Loss: 1.8176\n",
      "Epoch 013 Batch 000 Loss: 1.5518\n",
      "Epoch 013 Batch 001 Loss: 1.4789\n",
      "Epoch 013 Batch 002 Loss: 1.7933\n",
      "Epoch 013 Batch 003 Loss: 1.5127\n",
      "Epoch 013 Batch 004 Loss: 1.8676\n",
      "Epoch 013 Batch 005 Loss: 2.0162\n",
      "Epoch 013 Batch 006 Loss: 1.7974\n",
      "Epoch 013 Batch 007 Loss: 2.1139\n",
      "Epoch 013 Batch 008 Loss: 1.7260\n",
      "Epoch 013 Batch 009 Loss: 1.5283\n",
      "Epoch 014 Batch 000 Loss: 1.7428\n",
      "Epoch 014 Batch 001 Loss: 1.7379\n",
      "Epoch 014 Batch 002 Loss: 1.5650\n",
      "Epoch 014 Batch 003 Loss: 1.6304\n",
      "Epoch 014 Batch 004 Loss: 1.7750\n",
      "Epoch 014 Batch 005 Loss: 1.2195\n",
      "Epoch 014 Batch 006 Loss: 2.2375\n",
      "Epoch 014 Batch 007 Loss: 1.6041\n",
      "Epoch 014 Batch 008 Loss: 2.0837\n",
      "Epoch 014 Batch 009 Loss: 1.8531\n",
      "Epoch 015 Batch 000 Loss: 1.6593\n",
      "Epoch 015 Batch 001 Loss: 1.9657\n",
      "Epoch 015 Batch 002 Loss: 2.0225\n",
      "Epoch 015 Batch 003 Loss: 1.7549\n",
      "Epoch 015 Batch 004 Loss: 2.0974\n",
      "Epoch 015 Batch 005 Loss: 1.6252\n",
      "Epoch 015 Batch 006 Loss: 1.7189\n",
      "Epoch 015 Batch 007 Loss: 2.0099\n",
      "Epoch 015 Batch 008 Loss: 1.6141\n",
      "Epoch 015 Batch 009 Loss: 1.2921\n",
      "Epoch 016 Batch 000 Loss: 2.2273\n",
      "Epoch 016 Batch 001 Loss: 1.6490\n",
      "Epoch 016 Batch 002 Loss: 1.3572\n",
      "Epoch 016 Batch 003 Loss: 1.7282\n",
      "Epoch 016 Batch 004 Loss: 1.5787\n",
      "Epoch 016 Batch 005 Loss: 1.2644\n",
      "Epoch 016 Batch 006 Loss: 1.4513\n",
      "Epoch 016 Batch 007 Loss: 1.2593\n",
      "Epoch 016 Batch 008 Loss: 1.3286\n",
      "Epoch 016 Batch 009 Loss: 1.8334\n",
      "Epoch 017 Batch 000 Loss: 1.4329\n",
      "Epoch 017 Batch 001 Loss: 1.6502\n",
      "Epoch 017 Batch 002 Loss: 1.4748\n",
      "Epoch 017 Batch 003 Loss: 1.6352\n",
      "Epoch 017 Batch 004 Loss: 1.6722\n",
      "Epoch 017 Batch 005 Loss: 1.6942\n",
      "Epoch 017 Batch 006 Loss: 1.8163\n",
      "Epoch 017 Batch 007 Loss: 1.5057\n",
      "Epoch 017 Batch 008 Loss: 1.7295\n",
      "Epoch 017 Batch 009 Loss: 2.0485\n",
      "Epoch 018 Batch 000 Loss: 1.3982\n",
      "Epoch 018 Batch 001 Loss: 1.6635\n",
      "Epoch 018 Batch 002 Loss: 1.5765\n",
      "Epoch 018 Batch 003 Loss: 1.5471\n",
      "Epoch 018 Batch 004 Loss: 1.5458\n",
      "Epoch 018 Batch 005 Loss: 1.7278\n",
      "Epoch 018 Batch 006 Loss: 1.4366\n",
      "Epoch 018 Batch 007 Loss: 1.6584\n",
      "Epoch 018 Batch 008 Loss: 1.7087\n",
      "Epoch 018 Batch 009 Loss: 2.0814\n",
      "Epoch 019 Batch 000 Loss: 1.7242\n",
      "Epoch 019 Batch 001 Loss: 1.7922\n",
      "Epoch 019 Batch 002 Loss: 1.4315\n",
      "Epoch 019 Batch 003 Loss: 1.7252\n",
      "Epoch 019 Batch 004 Loss: 1.4986\n",
      "Epoch 019 Batch 005 Loss: 1.9225\n",
      "Epoch 019 Batch 006 Loss: 1.3601\n",
      "Epoch 019 Batch 007 Loss: 1.3585\n",
      "Epoch 019 Batch 008 Loss: 1.3367\n",
      "Epoch 019 Batch 009 Loss: 1.8987\n",
      "Epoch 020 Batch 000 Loss: 1.6724\n",
      "Epoch 020 Batch 001 Loss: 1.5197\n",
      "Epoch 020 Batch 002 Loss: 1.6468\n",
      "Epoch 020 Batch 003 Loss: 1.9757\n",
      "Epoch 020 Batch 004 Loss: 1.4855\n",
      "Epoch 020 Batch 005 Loss: 1.6978\n",
      "Epoch 020 Batch 006 Loss: 0.9733\n",
      "Epoch 020 Batch 007 Loss: 1.7017\n",
      "Epoch 020 Batch 008 Loss: 1.6808\n",
      "Epoch 020 Batch 009 Loss: 2.0520\n",
      "Epoch 021 Batch 000 Loss: 1.5767\n",
      "Epoch 021 Batch 001 Loss: 1.4270\n",
      "Epoch 021 Batch 002 Loss: 1.5825\n",
      "Epoch 021 Batch 003 Loss: 2.0086\n",
      "Epoch 021 Batch 004 Loss: 1.0652\n",
      "Epoch 021 Batch 005 Loss: 2.4331\n",
      "Epoch 021 Batch 006 Loss: 1.5098\n",
      "Epoch 021 Batch 007 Loss: 1.7222\n",
      "Epoch 021 Batch 008 Loss: 1.6835\n",
      "Epoch 021 Batch 009 Loss: 1.4720\n",
      "Epoch 022 Batch 000 Loss: 1.4678\n",
      "Epoch 022 Batch 001 Loss: 1.5277\n",
      "Epoch 022 Batch 002 Loss: 1.1610\n",
      "Epoch 022 Batch 003 Loss: 1.4790\n",
      "Epoch 022 Batch 004 Loss: 1.4950\n",
      "Epoch 022 Batch 005 Loss: 1.3725\n",
      "Epoch 022 Batch 006 Loss: 1.2560\n",
      "Epoch 022 Batch 007 Loss: 1.2292\n",
      "Epoch 022 Batch 008 Loss: 1.3060\n",
      "Epoch 022 Batch 009 Loss: 1.4288\n",
      "Epoch 023 Batch 000 Loss: 1.4261\n",
      "Epoch 023 Batch 001 Loss: 1.5522\n",
      "Epoch 023 Batch 002 Loss: 1.4692\n",
      "Epoch 023 Batch 003 Loss: 1.1692\n",
      "Epoch 023 Batch 004 Loss: 1.3567\n",
      "Epoch 023 Batch 005 Loss: 1.5918\n",
      "Epoch 023 Batch 006 Loss: 1.4416\n",
      "Epoch 023 Batch 007 Loss: 1.5259\n",
      "Epoch 023 Batch 008 Loss: 1.4670\n",
      "Epoch 023 Batch 009 Loss: 1.1981\n",
      "Epoch 024 Batch 000 Loss: 0.9056\n",
      "Epoch 024 Batch 001 Loss: 1.5283\n",
      "Epoch 024 Batch 002 Loss: 1.1315\n",
      "Epoch 024 Batch 003 Loss: 1.4980\n",
      "Epoch 024 Batch 004 Loss: 0.9718\n",
      "Epoch 024 Batch 005 Loss: 1.0173\n",
      "Epoch 024 Batch 006 Loss: 1.2138\n",
      "Epoch 024 Batch 007 Loss: 1.5251\n",
      "Epoch 024 Batch 008 Loss: 1.7317\n",
      "Epoch 024 Batch 009 Loss: 1.6556\n",
      "Epoch 025 Batch 000 Loss: 1.2225\n",
      "Epoch 025 Batch 001 Loss: 1.6689\n",
      "Epoch 025 Batch 002 Loss: 1.3662\n",
      "Epoch 025 Batch 003 Loss: 1.0738\n",
      "Epoch 025 Batch 004 Loss: 1.2898\n",
      "Epoch 025 Batch 005 Loss: 1.2229\n",
      "Epoch 025 Batch 006 Loss: 1.3092\n",
      "Epoch 025 Batch 007 Loss: 1.5643\n",
      "Epoch 025 Batch 008 Loss: 1.3909\n",
      "Epoch 025 Batch 009 Loss: 1.4940\n",
      "Epoch 026 Batch 000 Loss: 0.9833\n",
      "Epoch 026 Batch 001 Loss: 1.3559\n",
      "Epoch 026 Batch 002 Loss: 1.1968\n",
      "Epoch 026 Batch 003 Loss: 1.3012\n",
      "Epoch 026 Batch 004 Loss: 1.0893\n",
      "Epoch 026 Batch 005 Loss: 1.1192\n",
      "Epoch 026 Batch 006 Loss: 1.0963\n",
      "Epoch 026 Batch 007 Loss: 1.4680\n",
      "Epoch 026 Batch 008 Loss: 1.3773\n",
      "Epoch 026 Batch 009 Loss: 1.5920\n",
      "Epoch 027 Batch 000 Loss: 1.5299\n",
      "Epoch 027 Batch 001 Loss: 1.1625\n",
      "Epoch 027 Batch 002 Loss: 1.3524\n",
      "Epoch 027 Batch 003 Loss: 0.9136\n",
      "Epoch 027 Batch 004 Loss: 1.2076\n",
      "Epoch 027 Batch 005 Loss: 1.5263\n",
      "Epoch 027 Batch 006 Loss: 1.5476\n",
      "Epoch 027 Batch 007 Loss: 1.5844\n",
      "Epoch 027 Batch 008 Loss: 1.7334\n",
      "Epoch 027 Batch 009 Loss: 1.3822\n",
      "Epoch 028 Batch 000 Loss: 1.5271\n",
      "Epoch 028 Batch 001 Loss: 1.4835\n",
      "Epoch 028 Batch 002 Loss: 1.2514\n",
      "Epoch 028 Batch 003 Loss: 1.2440\n",
      "Epoch 028 Batch 004 Loss: 1.3413\n",
      "Epoch 028 Batch 005 Loss: 1.0961\n",
      "Epoch 028 Batch 006 Loss: 1.7157\n",
      "Epoch 028 Batch 007 Loss: 1.6172\n",
      "Epoch 028 Batch 008 Loss: 1.2080\n",
      "Epoch 028 Batch 009 Loss: 1.3182\n",
      "Epoch 029 Batch 000 Loss: 1.0822\n",
      "Epoch 029 Batch 001 Loss: 1.0345\n",
      "Epoch 029 Batch 002 Loss: 1.1032\n",
      "Epoch 029 Batch 003 Loss: 1.1376\n",
      "Epoch 029 Batch 004 Loss: 1.7694\n",
      "Epoch 029 Batch 005 Loss: 0.9840\n",
      "Epoch 029 Batch 006 Loss: 1.3447\n",
      "Epoch 029 Batch 007 Loss: 1.4932\n",
      "Epoch 029 Batch 008 Loss: 1.1654\n",
      "Epoch 029 Batch 009 Loss: 1.0728\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "his = train()\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(range(len(his)),\n",
    "            his,\n",
    "            c=np.array([255, 71, 90]) / 255.,\n",
    "            label='test acc')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Acc')\n",
    "plt.legend(loc=0)\n",
    "plt.title('acc')\n",
    "plt.savefig(\"../assets/acc.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(src_nodes, sample_num, neighbor_table):\n",
    "    \"\"\"\n",
    "    根据源节点一阶采样指定数量的邻居，有放回\n",
    "    :param src_nodes: 源节点\n",
    "    :param sample_num: 采用邻居节点数\n",
    "    :param neighbor_table: 邻接矩阵\n",
    "    :return: 返回所有源节点的采样邻居，二维列表，每个列表代表一个源节点的采样邻居\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for sid in src_nodes:\n",
    "        # 从节点的邻居中进行有放回地进行采样\n",
    "        neighbor_nodes = neighbor_table.getrow(sid).nonzero()   # 得到源节点的所有邻居，非零邻居\n",
    "        res = np.random.choice(np.array(neighbor_nodes).flatten(), size=sample_num) # 从所有邻居中采样sample_num个数\n",
    "        results.append(res)\n",
    "        # print(res)\n",
    "    return np.asarray(results).flatten()\n",
    "\n",
    "\n",
    "def multihop_sampling(src_nodes, sample_nums, neighbor_table):\n",
    "    \"\"\"\n",
    "    根据源节点进行多阶采样\n",
    "    :param src_nodes: 源节点\n",
    "    :param sample_nums: 采样邻居节点数\n",
    "    :param neighbor_table:  邻接矩阵\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    sampling_result = [src_nodes]\n",
    "    # print('sampling_result',sampling_result)\n",
    "    for k, hopk_num in enumerate(sample_nums):\n",
    "        hopk_result = sampling(sampling_result[k], hopk_num, neighbor_table)\n",
    "        sampling_result.append(hopk_result)\n",
    "    return sampling_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 训练过程tensor逐步分析\n",
    "# model.train()\n",
    "# acc_list = []\n",
    "# for e in range(EPOCHS):\n",
    "#     for batch in range(NUM_BATCH_PER_EPOCH):\n",
    "#         batch_src_index = np.random.choice(train_index,\n",
    "#                                             size=(BTACH_SIZE, ))    # (16,),np.random.choice是指随机抽取数字组成size大小的数组\n",
    "#         batch_src_label = torch.from_numpy(\n",
    "#             train_label[batch_src_index]).long().to(DEVICE) # (16)\n",
    "#         batch_sampling_result = multihop_sampling(batch_src_index,\n",
    "#                                                     NUM_NEIGHBORS_LIST,\n",
    "#                                                     data.adjacency_dict)    # list, length=3, item: array (16,)\n",
    "#         batch_sampling_x = [\n",
    "#             torch.from_numpy(x[idx]).float().to(DEVICE)\n",
    "#             for idx in batch_sampling_result\n",
    "#         ]   # list, length=3, item: array (16,1433)\n",
    "#         # print(batch_src_index.shape)\n",
    "#         # print(batch_src_label.shape)\n",
    "#         # print(batch_sampling_result[0])\n",
    "#         # print(batch_sampling_x[0])\n",
    "#         batch_train_logits = model(batch_sampling_x)\n",
    "#         loss = criterion(batch_train_logits, batch_src_label)\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         print(\"Epoch {:03d} Batch {:03d} Loss: {:.4f}\".format(\n",
    "#             e, batch, loss.item()))\n",
    "#     acc = test()\n",
    "#     acc_list.append(acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch3090",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e578467fa5c12cffc301a3bc3421e1911b67151edde074c28fc0dd02d3ed613c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
