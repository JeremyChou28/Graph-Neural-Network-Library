{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adjacent_matrix(distance_file: str,\n",
    "                        num_nodes: int,\n",
    "                        id_file: str = None,\n",
    "                        graph_type=\"connect\") -> np.array:\n",
    "    \"\"\"\n",
    "    construct adjacent matrix by csv file   根据PEMS数据集的csv文件来构建邻接矩阵\n",
    "    :param distance_file: path of csv file to save the distances between nodes  # csv文件路径\n",
    "    :param num_nodes: number of nodes in the graph   graph中节点个数\n",
    "    :param id_file: path of txt file to save the order of the nodes     \n",
    "    :param graph_type: [\"connect\", \"distance\"] if use weight, please set distance\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    A = np.zeros([int(num_nodes), int(num_nodes)])\n",
    "\n",
    "    if id_file:\n",
    "        with open(id_file, \"r\") as f_id:\n",
    "            node_id_dict = {\n",
    "                int(node_id): idx\n",
    "                for idx, node_id in enumerate(f_id.read().strip().split(\"\\n\"))\n",
    "            }\n",
    "\n",
    "            with open(distance_file, \"r\") as f_d:\n",
    "                f_d.readline()\n",
    "                reader = csv.reader(f_d)\n",
    "                for item in reader:\n",
    "                    if len(item) != 3:\n",
    "                        continue\n",
    "                    i, j, distance = int(item[0]), int(item[1]), float(item[2])\n",
    "                    if graph_type == \"connect\":\n",
    "                        A[node_id_dict[i], node_id_dict[j]] = 1.\n",
    "                        A[node_id_dict[j], node_id_dict[i]] = 1.\n",
    "                    elif graph_type == \"distance\":\n",
    "                        A[node_id_dict[i], node_id_dict[j]] = 1. / distance\n",
    "                        A[node_id_dict[j], node_id_dict[i]] = 1. / distance\n",
    "                    else:\n",
    "                        raise ValueError(\n",
    "                            \"graph type is not correct (connect or distance)\")\n",
    "        return A\n",
    "\n",
    "    with open(distance_file, \"r\") as f_d:\n",
    "        f_d.readline()\n",
    "        reader = csv.reader(f_d)\n",
    "        for item in reader:\n",
    "            if len(item) != 3:\n",
    "                continue\n",
    "            i, j, distance = int(item[0]), int(item[1]), float(item[2])\n",
    "\n",
    "            if graph_type == \"connect\":\n",
    "                A[i, j], A[j, i] = 1., 1.\n",
    "            elif graph_type == \"distance\":\n",
    "                A[i, j] = 1. / distance\n",
    "                A[j, i] = 1. / distance\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    \"graph type is not correct (connect or distance)\")\n",
    "\n",
    "    return A\n",
    "\n",
    "\n",
    "def get_flow_data(flow_file: str) -> np.array:\n",
    "    \"\"\"\n",
    "    parse npz to get flow data  读取npz文件得到交通流数据\n",
    "    :param flow_file: (N, T, D)\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    data = np.load(flow_file)\n",
    "    flow_data = data['data'].transpose([1, 0,\n",
    "                                        2])[:, :,\n",
    "                                            0][:, :,\n",
    "                                               np.newaxis]  # [N, T, D]  D = 1\n",
    "    return flow_data\n",
    "\n",
    "\n",
    "class PEMSDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data_path, num_nodes, divide_days, time_interval,\n",
    "                 history_length, train_mode):\n",
    "        \"\"\"\n",
    "        load processed data\n",
    "        :param data_path: [\"graph file name\" , \"flow data file name\"], path to save the data file names\n",
    "        :param num_nodes: number of nodes in graph\n",
    "        :param divide_days: [ days of train data, days of test data], list to divide the original data\n",
    "        :param time_interval: time interval between two traffic data records (mins)\n",
    "        :param history_length: length of history data to be used\n",
    "        :param train_mode: [\"train\", \"test\"]\n",
    "        \"\"\"\n",
    "        self.data_path = data_path\n",
    "        self.num_nodes = num_nodes\n",
    "        self.train_mode = train_mode\n",
    "        self.train_days = divide_days[0]\n",
    "        self.test_days = divide_days[1]\n",
    "        self.history_length = history_length  # 6\n",
    "        self.time_interval = time_interval  # 5 min\n",
    "        self.one_day_length = int(24 * 60 / self.time_interval)\n",
    "        self.graph = get_adjacent_matrix(distance_file=data_path[0],\n",
    "                                         num_nodes=num_nodes)\n",
    "        self.flow_norm, self.flow_data = self.pre_process_data(\n",
    "            data=get_flow_data(data_path[1]), norm_dim=1)\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.train_mode == \"train\":\n",
    "            return self.train_days * self.one_day_length - self.history_length\n",
    "        elif self.train_mode == \"test\":\n",
    "            return self.test_days * self.one_day_length\n",
    "        else:\n",
    "            raise ValueError(\"train mode: [{}] is not defined\".format(\n",
    "                self.train_mode))\n",
    "\n",
    "    def __getitem__(self, index):  # (x, y), index = [0, L1 - 1]\n",
    "        if self.train_mode == \"train\":\n",
    "            index = index\n",
    "        elif self.train_mode == \"test\":\n",
    "            index += self.train_days * self.one_day_length\n",
    "        else:\n",
    "            raise ValueError(\"train mode: [{}] is not defined\".format(\n",
    "                self.train_mode))\n",
    "\n",
    "        data_x, data_y = PEMSDataset.slice_data(self.flow_data,\n",
    "                                                self.history_length, index,\n",
    "                                                self.train_mode)\n",
    "        data_x = PEMSDataset.to_tensor(data_x)  # [N, H, D]\n",
    "        data_y = PEMSDataset.to_tensor(data_y).unsqueeze(1)  # [N, 1, D]\n",
    "        return {\n",
    "            \"graph\": PEMSDataset.to_tensor(self.graph),\n",
    "            \"flow_x\": data_x,\n",
    "            \"flow_y\": data_y\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def slice_data(data, history_length, index, train_mode):\n",
    "        \"\"\"\n",
    "        :param data: np.array, normalized traffic data.\n",
    "        :param history_length: int, length of history data to be used.\n",
    "        :param index: int, index on temporal axis.\n",
    "        :param train_mode: str, [\"train\", \"test\"].\n",
    "        :return:\n",
    "            data_x: np.array, [N, H, D].\n",
    "            data_y: np.array [N, D].\n",
    "        \"\"\"\n",
    "        if train_mode == \"train\":\n",
    "            start_index = index\n",
    "            end_index = index + history_length\n",
    "        elif train_mode == \"test\":\n",
    "            start_index = index - history_length\n",
    "            end_index = index\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"train model {} is not defined\".format(train_mode))\n",
    "\n",
    "        data_x = data[:, start_index:end_index]\n",
    "        data_y = data[:, end_index]\n",
    "\n",
    "        return data_x, data_y\n",
    "\n",
    "    @staticmethod\n",
    "    def pre_process_data(data, norm_dim):\n",
    "        \"\"\"\n",
    "        :param data: np.array, original traffic data without normalization.\n",
    "        :param norm_dim: int, normalization dimension.\n",
    "        :return:\n",
    "            norm_base: list, [max_data, min_data], data of normalization base.\n",
    "            norm_data: np.array, normalized traffic data.\n",
    "        \"\"\"\n",
    "        norm_base = PEMSDataset.normalize_base(\n",
    "            data, norm_dim)  # find the normalize base\n",
    "        norm_data = PEMSDataset.normalize_data(norm_base[0], norm_base[1],\n",
    "                                               data)  # normalize data\n",
    "\n",
    "        return norm_base, norm_data\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize_base(data, norm_dim):\n",
    "        \"\"\"\n",
    "        :param data: np.array, original traffic data without normalization.\n",
    "        :param norm_dim: int, normalization dimension.\n",
    "        :return:\n",
    "            max_data: np.array\n",
    "            min_data: np.array\n",
    "        \"\"\"\n",
    "        max_data = np.max(data, norm_dim,\n",
    "                          keepdims=True)  # [N, T, D] , norm_dim=1, [N, 1, D]\n",
    "        min_data = np.min(data, norm_dim, keepdims=True)\n",
    "        return max_data, min_data\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize_data(max_data, min_data, data):\n",
    "        \"\"\"\n",
    "        :param max_data: np.array, max data.\n",
    "        :param min_data: np.array, min data.\n",
    "        :param data: np.array, original traffic data without normalization.\n",
    "        :return:\n",
    "            np.array, normalized traffic data.\n",
    "        \"\"\"\n",
    "        mid = min_data\n",
    "        base = max_data - min_data\n",
    "        normalized_data = (data - mid) / base\n",
    "\n",
    "        return normalized_data\n",
    "\n",
    "    @staticmethod\n",
    "    def recover_data(max_data, min_data, data):\n",
    "        \"\"\"\n",
    "        :param max_data: np.array, max data.\n",
    "        :param min_data: np.array, min data.\n",
    "        :param data: np.array, normalized data.\n",
    "        :return:\n",
    "            recovered_data: np.array, recovered data.\n",
    "        \"\"\"\n",
    "        mid = min_data\n",
    "        base = max_data - min_data\n",
    "\n",
    "        recovered_data = data * base + mid\n",
    "\n",
    "        return recovered_data\n",
    "\n",
    "    @staticmethod\n",
    "    def to_tensor(data):\n",
    "        return torch.tensor(data, dtype=torch.float)\n",
    "\n",
    "\n",
    "def get_loader(ds_name=\"PEMS04\"):\n",
    "    num_nodes = 307 if ds_name == 'PEMS04' else 170\n",
    "    train_data = PEMSDataset(data_path=[\n",
    "        \"/home/zhoujianping/Research/GNN/TrafficPrediction/datasets/{}/distance.csv\".format(ds_name),\n",
    "        \"/home/zhoujianping/Research/GNN/TrafficPrediction/datasets/{}/data.npz\".format(ds_name)\n",
    "    ],\n",
    "                             num_nodes=num_nodes,\n",
    "                             divide_days=[45, 14],\n",
    "                             time_interval=5,\n",
    "                             history_length=6,\n",
    "                             train_mode=\"train\")\n",
    "\n",
    "    train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "\n",
    "    test_data = PEMSDataset(data_path=[\n",
    "        \"/home/zhoujianping/Research/GNN/TrafficPrediction/datasets/{}/distance.csv\".format(ds_name),\n",
    "        \"/home/zhoujianping/Research/GNN/TrafficPrediction/datasets/{}/data.npz\".format(ds_name)\n",
    "    ],\n",
    "                            num_nodes=num_nodes,\n",
    "                            divide_days=[45, 14],\n",
    "                            time_interval=5,\n",
    "                            history_length=6,\n",
    "                            train_mode=\"test\")\n",
    "\n",
    "    test_loader = DataLoader(test_data, batch_size=64, shuffle=False)\n",
    "    return train_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_flow(filename):\n",
    "    flow_data = np.load(filename)\n",
    "    # print(type(flow_data))\n",
    "    # PEMS数据集的data.npz文件中的data这个文件存放数据\n",
    "    return flow_data['data']\n",
    "\n",
    "def read_dataset(dataset):\n",
    "    data = np.load(dataset)\n",
    "    print(type(data['data']))\n",
    "    print(data['data'][0].shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "npy和npz文件的生成与保存方式：https://www.cnblogs.com/Lilu-1226/p/9768368.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(307, 16992, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.lib.npyio.NpzFile'>\n",
      "data size (16992, 307, 3)\n",
      "<class 'numpy.ndarray'>\n",
      "(307, 3)\n"
     ]
    }
   ],
   "source": [
    "dataset_file='/home/zhoujianping/Research/ASTGNN/data/PEMS04/PEMS04.npz'\n",
    "traffic_flow_data=get_flow_data(dataset_file)   \n",
    "print(traffic_flow_data.shape)  # (307,16992,1)只取flow这个维度，对tensor做了transform\n",
    "print(type(traffic_flow_data))\n",
    "\n",
    "\n",
    "traffic_data = get_flow(dataset_file)\n",
    "print(\"data size {}\".format(traffic_data.shape))    # (16992,307,3)其中16992代表时间长度 59天*24小时*12次（每5分钟采集一次数据），307是指检测器数量，3是指数据维度分别对应flow，occupy，speed\n",
    "\n",
    "read_dataset(dataset_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.utils.data.dataloader.DataLoader'>\n",
      "<class 'torch.utils.data.dataloader.DataLoader'>\n",
      "203\n",
      "63\n",
      "<class 'dict'>\n",
      "dict_keys(['graph', 'flow_x', 'flow_y'])\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([64, 307, 307])\n",
      "torch.Size([64, 307, 6, 1])\n",
      "torch.Size([64, 307, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "train_loader, test_loader = get_loader('PEMS04')\n",
    "print(type(train_loader))\n",
    "print(type(test_loader))\n",
    "\n",
    "print(len(train_loader))    # math.ceil(45*24*12/64)=203，原始数据集的前45天数据作为训练集，后面14天作为测试集\n",
    "print(len(test_loader))\n",
    "for data in train_loader:   # batch_size=64\n",
    "    # print(data)\n",
    "    print(type(data))\n",
    "    print(data.keys())\n",
    "    print(type(data['graph']))\n",
    "    print(type(data['flow_x']))\n",
    "    print(type(data['flow_y']))\n",
    "    print(data['graph'].shape)\n",
    "    print(data['flow_x'].shape) # flow_x表示history data，长度取6个\n",
    "    print(data['flow_y'].shape) # flow_y表示要预测的值\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (default, Nov 14 2022, 12:59:47) \n[GCC 9.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
